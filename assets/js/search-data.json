{
  
    
        "post0": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote [^1]. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | .",
            "url": "https://roberttorfason.github.io/blog/markdown/2021/09/22/test-markdown-post.html",
            "relUrl": "/markdown/2021/09/22/test-markdown-post.html",
            "date": " • Sep 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Election Questionnaire (Kosningapróf)",
            "content": "Introduction . The Icelandic parliament election is on the 25. September. Before every election, media outlets set up a quiz/questionnaire where candidates get statements such as &quot;Iceland should be a part of NATO&quot; and &quot;The Icelandic government should put more money into the healthcare system&quot; and the candidates answer if they agree/disagree with or are neutral towards the statement. Users can then answer the same questions and figure out which candidates and political parties they are &quot;closest to&quot; their political beliefs using the answers to the questions. . These are mostly for fun and should only serve as an indicator, but it&#39;s an enjoyable process to go through and it&#39;s always interesting to see which candidates are &quot;most similar&quot; to oneself. . As a whole this collection of data, candidates and their answers to a set of questions, is interesting and has a lot of opportunities for some data exploration and the purpose of this post is to take the data from the RUV quiz explore it and try to answer some questions about it. . Similar (and definitely more rigorous) analysis has been done before by people designing the tests and actualluy working with the data, see for example this great thread here on this quiz. Since this should not be taken too seriously, the analysis in this post will be more about generating plausible hypthes and doing some ad-hoc analysis. . If you want to fetch the data for yourself, e.g. to run this notebook locally follow the instructions here . The Data . Let&#39;s load the data, pre-process it and set up some helper objects . df_results, df_questions = pd.read_csv(&quot;results_2021.csv&quot;), pd.read_csv(&quot;questions_2021.csv&quot;) . # Pre-processing df_results[&quot;party&quot;] = df_results[&quot;party&quot;].astype(&quot;category&quot;) df_results[&quot;gender&quot;] = df_results[&quot;gender&quot;].astype(&quot;category&quot;) # Bin the ages. `pd.cut` returns intervals that are annoying to work with so we just use the # left age of each bin e.g. 30 to represent the interval [30, 40) age_binned_series = pd.cut(df_results[&quot;age&quot;], bins=[-10, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], right=False) df_results.insert(df_results.columns.get_loc(&quot;age&quot;) + 1, &quot;age_binned&quot;, age_binned_series) df_results[&quot;age_binned&quot;] = df_results[&quot;age_binned&quot;].map(lambda x: x.left).astype(&quot;category&quot;) # Most of the analysis centers around the political party so we drop the candiadates that don&#39;t have # a party specified df_results = df_results[~df_results[&quot;party&quot;].isna()] df_results = df_results.reset_index(drop=True) . . cols_questions = [c for c in df_results.columns if c.startswith(&quot;question_&quot;)] cols_meta = [c for c in df_results.columns if c not in cols_questions] question_id_to_string = dict(zip(df_questions[&quot;question_number&quot;], df_questions[&quot;question&quot;])) . . and take a look at the structure . df_questions.head(3) . question_number id question . 0 question_0 | 5719843612393472 | Íslenskt samfélag einkennist af réttlæti, sann... | . 1 question_1 | 5692341510733824 | Það á að leyfa kórónuveirunni að ganga án sótt... | . 2 question_2 | 5284747587616768 | Stjórnvöld eiga að setja strangar takmarkanir ... | . df_questions has all the questions and their ids/numbers. . df_results.head(3) . name party age age_binned gender timestamp constituency answering_done timestamp_dt question_0 ... question_21 question_22 question_23 question_24 question_25 question_26 question_27 question_28 question_29 question_30 . 0 Elín Tryggvadóttir | Samfylkingin | 46 | 40 | Kona | 1631051439022 | Reykjavíkurkjördæmi suður | 1.0 | 2021-09-07 21:50:39.022 | 51 | ... | 83 | 3 | 100 | 93 | 100 | 100 | 100 | 8 | 100 | 63 | . 1 Ágústa Anna Ómarsdóttir | Sósíalistaflokkurinn | 55 | 50 | Kona | 1631547337578 | Norðvesturkjördæmi | 1.0 | 2021-09-13 15:35:37.578 | 0 | ... | 0 | 27 | 61 | 61 | 96 | 100 | 100 | 0 | 100 | 32 | . 2 María Lilja Þrastardóttir Kemp | Sósíalistaflokkurinn | 35 | 30 | Kona | 1631030960104 | Reykjavíkurkjördæmi suður | 1.0 | 2021-09-07 16:09:20.104 | 22 | ... | 100 | 2 | 100 | 58 | 100 | 100 | 100 | 38 | 100 | 24 | . 3 rows × 40 columns . df_results represents each candidate in a row, metadata (age, party, name) and the results for all the questions, where each answer is on the scale from 0-100, 0 meaning that the candidate strongly disagrees with the statement and 100 means the candidate strongly agrees with the statement . &quot;&quot;&quot;This text is in a code cell because it&#39;s not possible to collapse markdown cells in fastpages Additionally each question in `df_results` has a mapping back to `df_questions` via the column name. Note that the way the questions are indexed there is an easy correspondance between the (numeric) index of each column and `df_questions`. This means that when we later transform the data to numpy arrays, where we don&#39;t have named columns, and do something like `x[:, 3]`, it will correspond to `df_questions.iloc[3]` so going back and forth between the data and the actual questions is easy. &quot;&quot;&quot;; . . Interactive Histogram of Questions and Answers . Below we visualize a histogram of the answers the candidates gave to the questions. The x-axis, Answer Value, is the value of the answer to each question binned and the y-axis is simply the count of those values. Again, these answers are on the scale 0-100, 0 meaning strongly disagree with the statement and 100 strongly agree with the statement. There are also two dropdown menus: One filters by political party and one filters by question so you can see the distribution of answers for each party and each question . The questions are ordered by most &quot;interesting&quot; to least &quot;interesting&quot;, where the standard deviation is used as proxy for how interesting it is. . &quot;&quot;&quot;This text is in a code cell because it&#39;s not possible to collapse markdown cells in fastpages Why does standard deviation make sense as a proxy for how interesting a question is? As a very informal argument, thinking about the different scenarios: 1. Everyone answers the same or a similar value -&gt; the std. will be low 2. The answers are (roughly) uniformly distributed over possible values -&gt; std. will be a &quot;medium&quot; value 3. If there is a strong split (bi-modal distribution) where candidates either agree or disagree with the statements -&gt; std. is high Visual inspection of the plots also supports this. One might be inclined to use entropy to measure how interesting a question is, but in that case the ordering would be 1. &lt; 3. &lt; 2., whichis not the desired outcome for this problem, so the std. is more appropriate her. &quot;&quot;&quot;; . . We need to pre-process the data for this plot, transforming it from a tall dataframe to a wide dataframe. See a good discussion on why that&#39;s useful here . df_results_melt = pd.melt(df_results, id_vars=cols_meta, value_vars=cols_questions) df_results_melt = df_results_melt[[&quot;party&quot;, &quot;variable&quot;, &quot;value&quot;]] df_results_melt = df_results_melt.rename(columns={&quot;variable&quot;: &quot;question&quot;, &quot;value&quot;: &quot;Answer Value&quot;}) df_results_melt[&quot;question&quot;] = df_results_melt[&quot;question&quot;].replace(question_id_to_string) df_results_melt[&quot;question&quot;] = df_results_melt[&quot;question&quot;].astype(&quot;category&quot;) . . df_questions_std = df_results_melt.groupby(&quot;question&quot;).std().sort_values(&quot;Answer Value&quot;, ascending=False) questions_sorted = df_questions_std.index.to_list() . Dimensionality Reduction and Embeddings . Principal Component Analysis (PCA) . Now we want to plot the data in a lower dimension so we run PCA on the data to get the 2 components that explain most of the variance in the data to be able to plot the candidates and their location in space using these new basis functions . First we pick out the questions from the dataframe and transform the extracted questions to a numpy array to be used with sklearn functions. Finally we normalize it to be in the range [0, 1] . df_questions_only = df_results.filter(like=&quot;question_&quot;) x = df_questions_only.to_numpy() x = x.astype(float) / 100 . pca = PCA(n_components=4) x_pca = pca.fit_transform(x) print(f&quot;Explained variance of each component {pca.explained_variance_ratio_}. n&quot; f&quot;Total explained variance of first 4 components {np.sum(pca.explained_variance_ratio_):.4f} n&quot; f&quot;Total explained variance of first 2 components {np.sum(pca.explained_variance_ratio_[:2]):.4f}&quot;) . Explained variance of each component [0.3562663 0.12366604 0.06636211 0.04447176]. Total explained variance of first 4 components 0.5908 Total explained variance of first 2 components 0.4799 . The first 2 components only explain roughly 50% of the variance in the data, but the ones that come after individually do not add a lot of explaining power. . We plot these components where you can see a breakdown by party and a tooltip overlay indicating which candidate corresponds to which point on the plot. . I don&#39;t want to interpret the components for the readers, that is a subjective process, but since the components are a linear combination of the question vectors, we can take a look at which questions contribute most strongly to each component so we can use that to help us interpret them. . Note that for negative (red) questions, we need to negate the statement to get the direction that aligns with the components that have positive (green) questions. . df_questions_and_components = pd.DataFrame( {&quot;Question&quot;: df_questions[&quot;question&quot;], &quot;Component 0&quot;: pca.components_[0], &quot;Component 1&quot;: pca.components_[1]} ) # We need to sort by the absolute value of each component s.t. we don&#39;t disregard components with a negative sign df_questions_and_components[&quot;Component 0 abs&quot;] = df_questions_and_components[&quot;Component 0&quot;].abs() df_questions_and_components[&quot;Component 1 abs&quot;] = df_questions_and_components[&quot;Component 1&quot;].abs() . . The top contributing questions for PCA component 0: . &nbsp; Component 0 Question . 3 0.319508 | Auka á vægi einkareksturs í heilbrigðiskerfinu. | . 8 0.267722 | Ísland á að eiga aðild að NATO. | . 14 -0.266694 | Hækka á skatta á tekjuhæsta fólkið. | . 25 -0.251547 | Alþingi á að vinna markvisst að endurskoðun stjórnarskrárinnar á næsta kjörtímabili. | . 23 -0.242049 | Ríkið á að styðja og taka þátt í fjármögnun Borgarlínu. | . 10 -0.238891 | Stjórnvöld eiga með beinum hætti að stuðla að samdrætti í losun gróðurhúsalofttegunda, til dæmis með hærri gjöldum á mengunarvalda. | . The top contributing questions for PCA component 1: . &nbsp; Component 1 Question . 28 -0.456681 | Leyfa á smásölu áfengis í matvöruverslunum. | . 22 0.348352 | Reykjavíkurflugvöllur á að vera í Vatnsmýri um ókomna framtíð. | . 7 -0.330895 | Efna á til þjóðaratkvæðagreiðslu um framhald aðildarviðræðna við ESB. | . 29 -0.301226 | Varsla neysluskammta af kannabisefnum ætti að vera refsilaus. | . 23 -0.273408 | Ríkið á að styðja og taka þátt í fjármögnun Borgarlínu. | . 21 -0.247242 | Innheimta á veggjöld í auknum mæli til að fjármagna nýjar vegaframkvæmdir. | . Non-negative Matrix Factorization (NMF) . PCA is not the only way to do dimensionality reduction. Another method is non-negative matrix factorization, whose purpose is not necesseraly dimensionality reduction, but the process gives us a natural lower dimensional representation of the candidates and the questions in 2 differnt spaces that have a similar structure. . Very briefly, NMF seeks to find 2 low rank non-negative matrices $W$ and $H$ such that $W cdot H approx X$, where $X$ in this case is our data matrix with the shape (n_candidates, n_questions), $W$ has the shape (n_candidates, n_components) and $H$ has the shape (n_components, n_questions). For each candidate and each question we get a representation in n_components dimensions, in our case we get a 2-dimensional vector. . &quot;&quot;&quot;This text is in a code cell because it&#39;s not possible to collapse markdown cells in fastpages We could also try doing PCA on x.T. The problem is that the basis functions are not interpretable (linear combinations of candidates) and the structure of the space does not necessarily have to correspond to the structure of the space found when doing PCA on the candidates. &quot;&quot;&quot;; . . nmf = NMF(n_components=2, init=&#39;random&#39;, max_iter=400, alpha=0.5, l1_ratio=0.5) W = nmf.fit_transform(x) H = nmf.components_ . Inspect the shapes of the results as a sanity check . x.shape = (n_candidates, n_questions) = (311, 31) W.shape = (n_candidates, n_components) = (311, 2) H.shape = (n_components, n_questions) = (2, 31) (W * H).shape = (n_candidates, n_questions) = (311, 31) . We get a space that seems to capture variability along a single dimension, but the components have a pretty clear meaning. But while the single dimension of variability has a clear interpretation, what does the smaller variability in the tangential dimension of this single dimension (distance from origin) mean? . Each element $x_{ij}$ in $X$ is approximated the inner product of a candidate vector and a question vector i.e. $w_i cdot h_j$. The higher this inner product is, the higher the value (higher agreement) the resulting answer. If we focus on a question, the interpretation of the radial distance is proportional to the ratio of candidates agree with the statement. . To support the hypothesis, let&#39;s select two questions whose embeddings are close and far away from the origin and look at the histogram they produce . Interpreting Questions by Classifying Targets . Thus far we have not used the &quot;target&quot; information i.e. &quot;party&quot; or &quot;age&quot; in our analysis. Both dimensionality reduction techniques learned a representation of the matrix $X$ without using the target data, except for visualization. . Another way to look at the questions is to train a classifier and see which questions are most important to distinguish between the target variables, &quot;party&quot; or &quot;age&quot;. Note that this is not the the same as calculating the variance of the questions as we did before. Even though these are related, a question might have high variance but would not help a classifier separate parties because each party could be divided on the question. This is not very likely, but it could happen in theory to some extent. . Let&#39;s look at classifying between the different parties using the questions . from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_validate clf = RandomForestClassifier() . We use the default RandomForestClassifier. We are not really interested in maximizing performance, we just want to see if there is a signal present . Filter the data and check the performance performance over 5 splits. . y, idxs = target_from_col_name(df_results, &quot;party&quot;) x_filter, y_filter = x[idxs, :], y[idxs] cv_results = cross_validate(clf, x_filter, y_filter, cv=5) f&quot;Accuracy = {np.mean(cv_results[&#39;test_score&#39;]):.3f}, Std. = {np.std(cv_results[&#39;test_score&#39;]):.4f}&quot; . &#39;Accuracy = 0.778, Std. = 0.0351&#39; . We see there is a strong signal present, as expected since these questions were designed explicitly to differentiate between political parties! We&#39;d expect a random classifier to have an accuracy of $1/11 approx 0.09$ (assuming uniform distribution of number of candidates per party for 11 parties) so this is clearly way better. . Let&#39;s take a look at the questions that are most important for classification between parties using RandomForestClassifier.feature_importance_. We don&#39;t really care about the numbers themselves, just the rank ordering. . df_questions_with_importance = most_important_questions(clf, x_filter, y_filter, df_questions) df_questions_with_importance[[&quot;importance&quot;, &quot;question&quot;]].head(6) . importance question . 3 0.102143 | Auka á vægi einkareksturs í heilbrigðiskerfinu. | . 7 0.083556 | Efna á til þjóðaratkvæðagreiðslu um framhald aðildarviðræðna við ESB. | . 21 0.057165 | Innheimta á veggjöld í auknum mæli til að fjármagna nýjar vegaframkvæmdir. | . 0 0.054635 | Íslenskt samfélag einkennist af réttlæti, sanngirni og jöfnum tækifærum. | . 8 0.051217 | Ísland á að eiga aðild að NATO. | . 28 0.043999 | Leyfa á smásölu áfengis í matvöruverslunum. | . And the least important . df_questions_with_importance.tail(6)[[&quot;importance&quot;, &quot;question&quot;]] . importance question . 18 0.017783 | Stjórnvöld ættu að innheimta hærri gjöld og skatta af erlendum ferðamönnum. | . 5 0.017002 | Draga ætti verulega úr kostnaðarþátttöku sjúklinga í heilbrigðiskerfinu. | . 19 0.016715 | Stjórnvöld ættu að flytja fleiri ríkisstofnanir til landsbyggðanna. | . 6 0.015550 | Ríkið ætti að taka meiri þátt í lyfjakostnaði sjúklinga. | . 26 0.010428 | Tryggja þarf að örorkubætur og ellilífeyrir séu ekki lægri en lægstu laun á vinnumarkaði. | . 27 0.006358 | Efla þarf velferðarkerfið svo það styðji betur við þá sem höllustum fæti standa í samfélaginu. | . We can do the same but using &quot;age&quot; as our target . y, idxs = target_from_col_name(df_results, &quot;age_binned&quot;, -10) x_filter, y_filter = x[idxs, :], y[idxs] cv_results = cross_validate(clf, x_filter, y_filter, cv=5) f&quot;Accuracy = {np.mean(cv_results[&#39;test_score&#39;]):.3f}, Std. = {np.std(cv_results[&#39;test_score&#39;]):.4f}&quot; . &#39;Accuracy = 0.295, Std. = 0.0464&#39; . The signal is not nearly as strong, but there is still some present. We&#39;d expect a random classifier to have accuracy of $1 / 7 approx 0.14$. . The most important questions to classify between age groups are . importance question . 0 0.052378 | Íslenskt samfélag einkennist af réttlæti, sanngirni og jöfnum tækifærum. | . 18 0.049105 | Stjórnvöld ættu að innheimta hærri gjöld og skatta af erlendum ferðamönnum. | . 2 0.044763 | Stjórnvöld eiga að setja strangar takmarkanir við landamæri til að tryggja að smit berist ekki til landsins. | . 28 0.043717 | Leyfa á smásölu áfengis í matvöruverslunum. | . 1 0.042306 | Það á að leyfa kórónuveirunni að ganga án sóttvarnatakmarkana. | . 24 0.041959 | Stjórnvöld eiga að halda áfram að styðja við atvinnulífið í kjölfar heimsfaraldursins til að koma til móts við fyrirtæki sem lentu í rekstrarvanda í faraldrinum. | . Conclusion . Hopefully you enjoyed this slightly scattered analysis of this data and learned something new. At least got a chance to play with the interactive plots. There are still a lot of things to explore in the data so I recommend fetching it for yourself and playing around with it .",
            "url": "https://roberttorfason.github.io/blog/data-science/election/machine-learning/2021/09/22/kosningaprof.html",
            "relUrl": "/data-science/election/machine-learning/2021/09/22/kosningaprof.html",
            "date": " • Sep 22, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://roberttorfason.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://roberttorfason.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}